{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 834,
     "status": "ok",
     "timestamp": 1604085287728,
     "user": {
      "displayName": "Abdulfatah Adeneye",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwwTkgOMGbA-QRk6klobTr8Aqxlub_7jKWXCJLvA=s64",
      "userId": "17752013653843449263"
     },
     "user_tz": -60
    },
    "id": "vhdyliqX3u9G"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ub7LYZkn3u9N"
   },
   "source": [
    "# Classifying Music Note sounds using Few Shot Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 794,
     "status": "ok",
     "timestamp": 1604085289594,
     "user": {
      "displayName": "Abdulfatah Adeneye",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwwTkgOMGbA-QRk6klobTr8Aqxlub_7jKWXCJLvA=s64",
      "userId": "17752013653843449263"
     },
     "user_tz": -60
    },
    "id": "AJfQJHZg4fCD",
    "outputId": "f4fc30fc-cca8-4f00-8b06-c68d42734570"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 801,
     "status": "ok",
     "timestamp": 1604085291847,
     "user": {
      "displayName": "Abdulfatah Adeneye",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwwTkgOMGbA-QRk6klobTr8Aqxlub_7jKWXCJLvA=s64",
      "userId": "17752013653843449263"
     },
     "user_tz": -60
    },
    "id": "nS19oxec3u9N"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "# Load various imports \n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda, Conv2D, UpSampling2D, MaxPooling2D\n",
    "from tensorflow.keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45qAhijR3u9T"
   },
   "source": [
    "#### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 875,
     "status": "ok",
     "timestamp": 1604087202357,
     "user": {
      "displayName": "Abdulfatah Adeneye",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwwTkgOMGbA-QRk6klobTr8Aqxlub_7jKWXCJLvA=s64",
      "userId": "17752013653843449263"
     },
     "user_tz": -60
    },
    "id": "x1YSn5ic3u9U"
   },
   "outputs": [],
   "source": [
    "def fft(f):\n",
    "    Ni = len(f)\n",
    "    Mi = int(Ni / 2)\n",
    "    if Mi <= 2:\n",
    "        return [f[0] + f[1] + f[2] + f[3], \n",
    "               f[0] - 1j*f[1] - f[2] + 1j*f[3],\n",
    "               f[0] - f[1] + f[2] - f[3],\n",
    "               f[0] + 1j*f[1] - f[2] - 1j*f[3]]\n",
    "    \n",
    "    wn = math.cos(2*math.pi/Ni) - 1j*math.sin(2*math.pi/Ni)\n",
    "    fe = [f[i] for i in range(Ni) if i % 2 == 0]\n",
    "    fo = [f[i] for i in range(Ni) if i % 2 == 1]\n",
    "    Fe = fft(fe)\n",
    "    Fo = fft(fo)\n",
    "    return [np.around(Fe[i] + (wn**i)*Fo[i], decimals=10) for i in range(Mi)] + [np.around(Fe[i] - (wn**i)*Fo[i], decimals=10) for i in range(Mi)]\n",
    "\n",
    "def get_audio_data(filename):\n",
    "    fs = 2**12 # sample rate\n",
    "    tp = 2 # sampling duration\n",
    "    N = n = fs*tp # number of samples\n",
    "    \n",
    "    # Extract data and sampling rate from file\n",
    "    recording, fs = librosa.load(filename, sr=fs, duration=tp, mono=True)\n",
    "\n",
    "    n = len(recording)        \n",
    "    tp = int(n / fs)\n",
    "\n",
    "    if tp < 2:\n",
    "        pad_width = N - recording.shape[0]\n",
    "        recording = np.pad(recording, pad_width=((0, pad_width),), mode='constant')\n",
    "\n",
    "        n = len(recording)\n",
    "        tp = int(n / fs)\n",
    "\n",
    "    N = fs*tp # number of samples\n",
    "    x = [np.round(float(recording[i]), 10) for i in range(n)] # input sequence\n",
    "    return x, tp, n\n",
    "\n",
    "def get_frequency_amplitude(x, tp, N):\n",
    "    _X = fft(x) # discrete Fourier transform\n",
    "    X = [np.round(Xi/N, 10) for Xi in _X] # frequency spectrum\n",
    "    X_amp = [np.absolute(Xi) for Xi in X] # amplitude spectrum\n",
    "\n",
    "    M = int(N/2)\n",
    "    ti = [i*tp/N for i in range(N)]\n",
    "    fi = [i/tp for i in range(M)]\n",
    "    X_amp = np.array(X_amp[:M])*2\n",
    "    \n",
    "    return ti, fi, X_amp\n",
    "\n",
    "def extract_features(filepath):\n",
    "    # try:\n",
    "    audio_features = get_audio_data(filepath)\n",
    "    if not audio_features:\n",
    "        return\n",
    "\n",
    "    x, tp, N = audio_features\n",
    "    ti, fi, X_amp = get_frequency_amplitude(x, tp, N)\n",
    "    return X_amp\n",
    "#     return fi, X_amp\n",
    "    \n",
    "    # except Exception as e:\n",
    "    #     print(\"Error encountered while parsing file: \", file_name, e)\n",
    "    #     return None \n",
    "    \n",
    "def extract_features(file_name):\n",
    "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast', duration=3) \n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    \n",
    "    pad_width = 256 - mfccs.shape[1]\n",
    "    mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')     \n",
    "    \n",
    "#     mfccs = mfccs.mean(1)\n",
    "    return mfccs\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n",
    "                              mean=0., stddev=0.1)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZ6ISFg-3u9Y"
   },
   "source": [
    "#### Load Preprocessed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406,
     "referenced_widgets": [
      "e46f85e99e09488bae8eb6d70666e4ef",
      "02fa5eedbf8c403c94e45735e8e2072e",
      "cc12aa92344d4fd885d91aafeda3da2a",
      "32033641990b41f0bbee1cb4eb29f795",
      "8b729491eac44963bc4949b2d3ffda1a",
      "0fb775d8199642539df1f477850b9bb8",
      "01870c47bcd4473ea6d4a75a5671983c",
      "ab984203a3b64540aaf15530cd751c56"
     ]
    },
    "executionInfo": {
     "elapsed": 907,
     "status": "error",
     "timestamp": 1604087204042,
     "user": {
      "displayName": "Abdulfatah Adeneye",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwwTkgOMGbA-QRk6klobTr8Aqxlub_7jKWXCJLvA=s64",
      "userId": "17752013653843449263"
     },
     "user_tz": -60
    },
    "id": "wR58CuvV3u9Y",
    "outputId": "8874ee84-9086-4b8c-9028-ddf902d6d4bc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94ebeaf18bb45068ee6cf3abacd1ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished feature extraction from  47  files\n"
     ]
    }
   ],
   "source": [
    "# Set the path to the full UrbanSound dataset \n",
    "DATA_DIR = os.path.join(\"data\", \"guitar_sample\")\n",
    "# DATA_DIR = os.path.join(\"/content/drive/My Drive/Colab Notebooks/data\", \"guitar_sample\")\n",
    "\n",
    "# feature list\n",
    "features = []\n",
    "\n",
    "labels = os.listdir(DATA_DIR)\n",
    "\n",
    "# Iterate through each sound file and extract the features \n",
    "for folder in tqdm(labels):\n",
    "    for file in os.listdir(os.path.join(DATA_DIR, folder)):\n",
    "        class_label = folder\n",
    "        file_name = os.path.join(os.path.join(DATA_DIR, folder, file))\n",
    "        \n",
    "        data = extract_features(file_name)\n",
    "        if data is None:\n",
    "            continue\n",
    "        \n",
    "        data = np.array(data)\n",
    "        data = np.expand_dims(data, axis=-1)\n",
    "        features.append([data, class_label])\n",
    "\n",
    "# Convert into a Panda dataframe \n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "\n",
    "print('Finished feature extraction from ', len(featuresdf), ' files') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 256, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# featuresdf.feature = featuresdf.feature.apply(lambda xx: xx.reshape((4096, 2)))\n",
    "featuresdf.feature.iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "phknq-qI3u9d",
    "outputId": "7742269a-54bf-43d4-afe4-c53ea9737531"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[-403.97342], [-427.74152], [-478.33362], [-...</td>\n",
       "      <td>1A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[-449.60074], [-470.32797], [-528.5974], [-5...</td>\n",
       "      <td>1A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[-374.77917], [-398.41254], [-462.8929], [-4...</td>\n",
       "      <td>1A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[-400.76926], [-423.18558], [-481.08386], [-...</td>\n",
       "      <td>1A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[-391.5319], [-416.02634], [-477.748], [-511...</td>\n",
       "      <td>1A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature class_label\n",
       "0  [[[-403.97342], [-427.74152], [-478.33362], [-...          1A\n",
       "1  [[[-449.60074], [-470.32797], [-528.5974], [-5...          1A\n",
       "2  [[[-374.77917], [-398.41254], [-462.8929], [-4...          1A\n",
       "3  [[[-400.76926], [-423.18558], [-481.08386], [-...          1A\n",
       "4  [[[-391.5319], [-416.02634], [-477.748], [-511...          1A"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "qq7qH_Eh3u9m",
    "outputId": "02734d1c-465a-479e-f383-503415d5f3b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0A', '0B', '0D', '0EH', '0EL', '0G', '1A', '1B', '1D', '1EH', '1EL', '1G']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "T9FB6PBB3u9r",
    "outputId": "1759fa47-1cd9-4bad-fe2b-af5260cd2465"
   },
   "outputs": [],
   "source": [
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "input_data = np.array(featuresdf.feature.tolist())\n",
    "output_label = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# split train and test data\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=100)\n",
    "for train_index, test_index in sss.split(input_data, output_label):\n",
    "    x_train, x_test = input_data[train_index], input_data[test_index]\n",
    "    y_train_label, y_test_label = output_label[train_index], output_label[test_index]\n",
    "    \n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "y_train = le.fit_transform(y_train_label)\n",
    "y_test = le.transform(y_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "lJv29W0y3u91",
    "outputId": "192dd052-b77e-42c0-a50b-74d4e264f5d8",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35, 40, 256, 1), (35,))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Ri3TLncR3u94",
    "outputId": "90a6c723-2e0f-4a17-e348-8f5f5703b7a9",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12, 40, 256, 1), (12,))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "PhF_lk3g3u98",
    "outputId": "fbdeeee8-c0b5-416e-db4b-e8e687e4aa76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class =   0, label = 0A \n",
      "class =   0, label = 0A \n",
      "class =   0, label = 0A \n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"class = {y_train[i]:>3d}, label = {y_train_label[i]:3s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtEvp-x53u9_"
   },
   "source": [
    "### VAE model architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train.astype('float32') / x_train.max()\n",
    "# x_test = x_test.astype('float32') / x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "HqmrjFhq3u9i"
   },
   "outputs": [],
   "source": [
    "input_img = Input(shape=x_train.shape[1:])\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "encoder = Model(input_img, encoded, name='encoder')\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "# Create decoder\n",
    "latent_dim = tuple(encoded.shape[1:])\n",
    "latent_inputs = Input(shape=latent_dim, name='latent_space')\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(latent_inputs)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "decoder = Model(latent_inputs, decoded, name='decoder')\n",
    "\n",
    "autoencoder = Model(input_img, decoder(encoder(input_img)))\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 40, 256, 1)]      0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 40, 256, 16)       160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 20, 128, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 20, 128, 8)        1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 10, 64, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 10, 64, 8)         584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 5, 32, 8)          0         \n",
      "=================================================================\n",
      "Total params: 1,904\n",
      "Trainable params: 1,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "latent_space (InputLayer)    [(None, 5, 32, 8)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 5, 32, 8)          584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 10, 64, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 10, 64, 8)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2 (None, 20, 128, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 20, 128, 16)       1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2 (None, 40, 256, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 40, 256, 1)        145       \n",
      "=================================================================\n",
      "Total params: 2,481\n",
      "Trainable params: 2,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 40, 256, 1)]      0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         (None, 5, 32, 8)          1904      \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 40, 256, 1)        2481      \n",
      "=================================================================\n",
      "Total params: 4,385\n",
      "Trainable params: 4,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixLSgTDl3u-P"
   },
   "source": [
    "### Training \n",
    "\n",
    "Here we will train the model. As training a CNN can take a sigificant amount of time, we will start with a low number of epochs and a low batch size. If we can see from the output that the model is converging, we will increase both numbers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "rvVo5MbA3u-Q",
    "outputId": "cb3f5a7c-c9cc-4dba-847d-999509bfb5f9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 1s 827ms/step - loss: -1.6091 - val_loss: -5.8549\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 158ms/step - loss: -5.9562 - val_loss: -10.4401\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 340ms/step - loss: -10.7279 - val_loss: -14.9159\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 215ms/step - loss: -15.3384 - val_loss: -19.9632\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 160ms/step - loss: -20.5218 - val_loss: -26.0541\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 233ms/step - loss: -26.7427 - val_loss: -33.6940\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 207ms/step - loss: -34.5093 - val_loss: -43.1569\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 200ms/step - loss: -44.1672 - val_loss: -54.4939\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 191ms/step - loss: -55.7056 - val_loss: -67.9633\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 153ms/step - loss: -69.3998 - val_loss: -84.0537\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 259ms/step - loss: -85.7152 - val_loss: -103.0972\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 126ms/step - loss: -105.0964 - val_loss: -125.4784\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 190ms/step - loss: -127.9419 - val_loss: -151.7974\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: -154.8817 - val_loss: -182.3852\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 151ms/step - loss: -186.1490 - val_loss: -218.3250\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 189ms/step - loss: -222.7554 - val_loss: -260.6165\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 193ms/step - loss: -265.8243 - val_loss: -310.2164\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 420ms/step - loss: -316.3905 - val_loss: -368.3404\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 179ms/step - loss: -375.6403 - val_loss: -436.4427\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 190ms/step - loss: -445.1135 - val_loss: -515.2863\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 200ms/step - loss: -525.3979 - val_loss: -607.0958\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 249ms/step - loss: -619.1113 - val_loss: -714.1768\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 182ms/step - loss: -728.5931 - val_loss: -838.2097\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 183ms/step - loss: -855.4202 - val_loss: -982.1671\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 145ms/step - loss: -1002.6013 - val_loss: -1149.9485\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 154ms/step - loss: -1173.9971 - val_loss: -1345.7433\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 271ms/step - loss: -1374.0946 - val_loss: -1573.2831\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 144ms/step - loss: -1606.5625 - val_loss: -1837.9727\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 178ms/step - loss: -1877.1364 - val_loss: -2145.9419\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 199ms/step - loss: -2192.1914 - val_loss: -2504.0515\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 152ms/step - loss: -2558.2698 - val_loss: -2919.3157\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 197ms/step - loss: -2982.6411 - val_loss: -3401.0144\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 163ms/step - loss: -3474.7236 - val_loss: -3955.0271\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 276ms/step - loss: -4040.4692 - val_loss: -4598.7681\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 319ms/step - loss: -4698.1387 - val_loss: -5342.8042\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 244ms/step - loss: -5458.3076 - val_loss: -6201.8706\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 178ms/step - loss: -6336.6538 - val_loss: -7194.8804\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 196ms/step - loss: -7351.7314 - val_loss: -8342.4912\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 263ms/step - loss: -8524.3867 - val_loss: -9664.5049\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 226ms/step - loss: -9874.2988 - val_loss: -11183.4463\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 206ms/step - loss: -11424.1289 - val_loss: -12927.3955\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 353ms/step - loss: -13204.5703 - val_loss: -14926.3701\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 395ms/step - loss: -15244.6699 - val_loss: -17223.7969\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 458ms/step - loss: -17589.7969 - val_loss: -19855.3711\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 339ms/step - loss: -20275.5723 - val_loss: -22875.1504\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 207ms/step - loss: -23358.4531 - val_loss: -26327.8047\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 241ms/step - loss: -26884.2715 - val_loss: -30278.0059\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 242ms/step - loss: -30919.8711 - val_loss: -34797.9531\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 162ms/step - loss: -35534.1992 - val_loss: -39983.4805\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 255ms/step - loss: -40835.7461 - val_loss: -45909.1758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15920a569a0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "autoencoder.fit(\n",
    "    x_train, \n",
    "    x_train,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    validation_data=(x_test, x_test),\n",
    "    callbacks=[TensorBoard(log_dir='./tmp/autoencoder')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-n0tMCh3u-Y"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_encoded = np.array(encoder.predict(x_train, batch_size=1000))\n",
    "x_test_encoded = np.array(encoder.predict(x_test, batch_size=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6, 6))\n",
    "# plt.scatter(x_train_encoded[0, :, 0], x_train_encoded[0, :, 1], c=y_train)\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster close points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = make_pipeline(SVC(kernel=\"poly\", gamma='auto'))\n",
    "\n",
    "xx_train = x_train_encoded.reshape(len(x_train_encoded), -1)\n",
    "xx_test = x_test_encoded.reshape(len(x_test_encoded), -1)\n",
    "\n",
    "clf.fit(xx_train, y_train)\n",
    "clf.score(xx_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(xx_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 2],\n",
       "       [0, 5]], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(xx_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.71      0.83         7\n",
      "           1       0.71      1.00      0.83         5\n",
      "\n",
      "    accuracy                           0.83        12\n",
      "   macro avg       0.86      0.86      0.83        12\n",
      "weighted avg       0.88      0.83      0.83        12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1280)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_label = \"A\"\n",
    "sample_filepath = os.path.join(\"data\", \"sampleA.wav\")\n",
    "sample_vector = extract_features(sample_filepath)\n",
    "sample_vector = np.expand_dims(np.expand_dims(sample_vector, axis=-1), axis=0)\n",
    "sample_embedded = encoder.predict(sample_vector)\n",
    "sample_embedded = sample_embedded.reshape(len(sample_embedded), -1)\n",
    "sample_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0A'], dtype='<U2')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(clf.predict(sample_embedded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.predict_proba(sample_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2873ea858ddc456e92140e05ab386be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1EH, 1D, 1A, 1B, 1G, 1EL, \n",
      "\n",
      "(118, 1280)\n"
     ]
    }
   ],
   "source": [
    "x_test_sample = []\n",
    "y_test_sample = []\n",
    "\n",
    "sample_dirs = [\"1EH\", \"1D\", \"1A\", \"1B\", \"1G\", \"1EL\"]\n",
    "\n",
    "for label in tqdm(sample_dirs):\n",
    "    print(label, end=\", \")\n",
    "    labeldir= os.path.join(DATA_DIR, label)\n",
    "\n",
    "    for filename in (os.listdir(labeldir)):\n",
    "        anchor_filepath = os.path.join(DATA_DIR, label, filename)\n",
    "        \n",
    "        anchor_file_vector = extract_features(anchor_filepath)\n",
    "        anchor_file_vector = np.expand_dims(anchor_file_vector, axis=-1)\n",
    "        x_test_sample.append(anchor_file_vector)\n",
    "        y_test_sample.append(\"1A\" if label == \"1A\" else \"0A\")\n",
    "\n",
    "print()\n",
    "x_test_sample = encoder.predict(np.array(x_test_sample))\n",
    "y_test_sample = np.array(y_test_sample)\n",
    "\n",
    "x_test_sample = x_test_sample.reshape(len(x_test_sample), -1)\n",
    "y_test_sample = le.transform(y_test_sample)\n",
    "\n",
    "\n",
    "print(x_test_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4745762711864407"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_test_sample, y_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[36, 62],\n",
       "       [ 0, 20]], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(x_test_sample)\n",
    "confusion_matrix(y_test_sample, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.37      0.54        98\n",
      "           1       0.24      1.00      0.39        20\n",
      "\n",
      "    accuracy                           0.47       118\n",
      "   macro avg       0.62      0.68      0.46       118\n",
      "weighted avg       0.87      0.47      0.51       118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test_sample, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Triplet Siamese Network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01870c47bcd4473ea6d4a75a5671983c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "02fa5eedbf8c403c94e45735e8e2072e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0fb775d8199642539df1f477850b9bb8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32033641990b41f0bbee1cb4eb29f795": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab984203a3b64540aaf15530cd751c56",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_01870c47bcd4473ea6d4a75a5671983c",
      "value": " 0/12 [00:00&lt;?, ?it/s]"
     }
    },
    "8b729491eac44963bc4949b2d3ffda1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ab984203a3b64540aaf15530cd751c56": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc12aa92344d4fd885d91aafeda3da2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "  0%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fb775d8199642539df1f477850b9bb8",
      "max": 12,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b729491eac44963bc4949b2d3ffda1a",
      "value": 0
     }
    },
    "e46f85e99e09488bae8eb6d70666e4ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc12aa92344d4fd885d91aafeda3da2a",
       "IPY_MODEL_32033641990b41f0bbee1cb4eb29f795"
      ],
      "layout": "IPY_MODEL_02fa5eedbf8c403c94e45735e8e2072e"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
