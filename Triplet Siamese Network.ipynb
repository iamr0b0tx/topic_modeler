{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 834,
     "status": "ok",
     "timestamp": 1604085287728,
     "user": {
      "displayName": "Abdulfatah Adeneye",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwwTkgOMGbA-QRk6klobTr8Aqxlub_7jKWXCJLvA=s64",
      "userId": "17752013653843449263"
     },
     "user_tz": -60
    },
    "id": "vhdyliqX3u9G"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ub7LYZkn3u9N"
   },
   "source": [
    "# Classifying Music Note sounds using Few Shot Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 794,
     "status": "ok",
     "timestamp": 1604085289594,
     "user": {
      "displayName": "Abdulfatah Adeneye",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwwTkgOMGbA-QRk6klobTr8Aqxlub_7jKWXCJLvA=s64",
      "userId": "17752013653843449263"
     },
     "user_tz": -60
    },
    "id": "AJfQJHZg4fCD",
    "outputId": "f4fc30fc-cca8-4f00-8b06-c68d42734570"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 801,
     "status": "ok",
     "timestamp": 1604085291847,
     "user": {
      "displayName": "Abdulfatah Adeneye",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwwTkgOMGbA-QRk6klobTr8Aqxlub_7jKWXCJLvA=s64",
      "userId": "17752013653843449263"
     },
     "user_tz": -60
    },
    "id": "nS19oxec3u9N"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45qAhijR3u9T"
   },
   "source": [
    "#### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "executionInfo": {
     "elapsed": 875,
     "status": "ok",
     "timestamp": 1604087202357,
     "user": {
      "displayName": "Abdulfatah Adeneye",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwwTkgOMGbA-QRk6klobTr8Aqxlub_7jKWXCJLvA=s64",
      "userId": "17752013653843449263"
     },
     "user_tz": -60
    },
    "id": "x1YSn5ic3u9U"
   },
   "outputs": [],
   "source": [
    "def fft(f):\n",
    "    Ni = len(f)\n",
    "    Mi = int(Ni / 2)\n",
    "    if Mi <= 2:\n",
    "        return [f[0] + f[1] + f[2] + f[3], \n",
    "               f[0] - 1j*f[1] - f[2] + 1j*f[3],\n",
    "               f[0] - f[1] + f[2] - f[3],\n",
    "               f[0] + 1j*f[1] - f[2] - 1j*f[3]]\n",
    "    \n",
    "    wn = math.cos(2*math.pi/Ni) - 1j*math.sin(2*math.pi/Ni)\n",
    "    fe = [f[i] for i in range(Ni) if i % 2 == 0]\n",
    "    fo = [f[i] for i in range(Ni) if i % 2 == 1]\n",
    "    Fe = fft(fe)\n",
    "    Fo = fft(fo)\n",
    "    return [np.around(Fe[i] + (wn**i)*Fo[i], decimals=10) for i in range(Mi)] + [np.around(Fe[i] - (wn**i)*Fo[i], decimals=10) for i in range(Mi)]\n",
    "\n",
    "def get_audio_data(filename):\n",
    "    fs = 2**12 # sample rate\n",
    "    tp = 2 # sampling duration\n",
    "    N = n = fs*tp # number of samples\n",
    "    \n",
    "    # Extract data and sampling rate from file\n",
    "    recording, fs = librosa.load(filename, sr=fs, duration=tp)\n",
    "    recording = recording.reshape((-1, 1))\n",
    "\n",
    "    n = len(recording)        \n",
    "    tp = int(n / fs)\n",
    "\n",
    "    if tp < 2:\n",
    "        pad_width = N - recording.shape[0]\n",
    "        recording = np.pad(recording, pad_width=((0, pad_width), (0, 0)), mode='constant')\n",
    "\n",
    "        n = len(recording)\n",
    "        tp = int(n / fs)\n",
    "\n",
    "    N = fs*tp # number of samples\n",
    "    x = [round(float(recording[i]), 10) for i in range(n)] # input sequence\n",
    "    return x, tp, n\n",
    "\n",
    "def get_frequency_amplitude(x, tp, N):\n",
    "    _X = fft(x) # discrete Fourier transform\n",
    "    X = [round(Xi/N, 10) for Xi in _X] # frequency spectrum\n",
    "    X_amp = [np.absolute(Xi) for Xi in X] # amplitude spectrum\n",
    "\n",
    "    M = int(N/2)\n",
    "    ti = [i*tp/N for i in range(N)]\n",
    "    fi = [i/tp for i in range(M)]\n",
    "    X_amp = np.array(X_amp[:M])*2\n",
    "    \n",
    "    return ti, fi, X_amp\n",
    "\n",
    "def extract_features(filepath):\n",
    "    # try:\n",
    "    audio_features = get_audio_data(filepath)\n",
    "    if not audio_features:\n",
    "        return\n",
    "\n",
    "    x, tp, N = audio_features\n",
    "    ti, fi, X_amp = get_frequency_amplitude(x, tp, N)\n",
    "    return X_amp\n",
    "    \n",
    "    # except Exception as e:\n",
    "    #     print(\"Error encountered while parsing file: \", file_name, e)\n",
    "    #     return None \n",
    "    \n",
    "# def extract_features(file_name):\n",
    "   \n",
    "# #     try:\n",
    "#     audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "#     mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "#     mfccsscaled = np.mean(mfccs.T, axis=0)\n",
    "        \n",
    "# #     except Exception as e:\n",
    "# #         print(\"Error encountered while parsing file: \", file, e)\n",
    "# #         return None \n",
    "     \n",
    "#     return mfccsscaled\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZ6ISFg-3u9Y"
   },
   "source": [
    "#### Load Preprocessed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406,
     "referenced_widgets": [
      "e46f85e99e09488bae8eb6d70666e4ef",
      "02fa5eedbf8c403c94e45735e8e2072e",
      "cc12aa92344d4fd885d91aafeda3da2a",
      "32033641990b41f0bbee1cb4eb29f795",
      "8b729491eac44963bc4949b2d3ffda1a",
      "0fb775d8199642539df1f477850b9bb8",
      "01870c47bcd4473ea6d4a75a5671983c",
      "ab984203a3b64540aaf15530cd751c56"
     ]
    },
    "executionInfo": {
     "elapsed": 907,
     "status": "error",
     "timestamp": 1604087204042,
     "user": {
      "displayName": "Abdulfatah Adeneye",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhwwTkgOMGbA-QRk6klobTr8Aqxlub_7jKWXCJLvA=s64",
      "userId": "17752013653843449263"
     },
     "user_tz": -60
    },
    "id": "wR58CuvV3u9Y",
    "outputId": "8874ee84-9086-4b8c-9028-ddf902d6d4bc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d8a307d1e845749b68508d63993c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished feature extraction from  251  files\n"
     ]
    }
   ],
   "source": [
    "# Load various imports \n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "# Set the path to the full UrbanSound dataset \n",
    "DATA_DIR = os.path.join(\"data\", \"guitar_sample\")\n",
    "# DATA_DIR = os.path.join(\"/content/drive/My Drive/Colab Notebooks/data\", \"guitar_sample\")\n",
    "\n",
    "# feature list\n",
    "features = []\n",
    "\n",
    "# Iterate through each sound file and extract the features \n",
    "for folder in tqdm(os.listdir(DATA_DIR)):\n",
    "    for file in os.listdir(os.path.join(DATA_DIR, folder)):\n",
    "        class_label = folder\n",
    "        file_name = os.path.join(os.path.join(DATA_DIR, folder, file))\n",
    "        \n",
    "        data = extract_features(file_name)\n",
    "        if data is None:\n",
    "            continue\n",
    "        \n",
    "        data = np.array(data).reshape((64, 64))\n",
    "#         pad_width = (96*96*3) - data.shape[0]\n",
    "        \n",
    "#         data = np.pad(data, pad_width=((0, pad_width), (0, 0)), mode='constant')\n",
    "        features.append([data, class_label])\n",
    "\n",
    "# Convert into a Panda dataframe \n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "\n",
    "print('Finished feature extraction from ', len(featuresdf), ' files') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "phknq-qI3u9d",
    "outputId": "7742269a-54bf-43d4-afe4-c53ea9737531"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[7.2154e-06, 1.1226262854574536e-05, 9.577476...</td>\n",
       "      <td>0A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[1.25874e-05, 2.9857376931003163e-05, 4.90201...</td>\n",
       "      <td>0A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[5.00442e-05, 1.584144500100922e-05, 3.131867...</td>\n",
       "      <td>0A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[5.59316e-05, 2.868965890978838e-05, 2.260507...</td>\n",
       "      <td>0A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[2.30266e-05, 1.0840161257103143e-05, 2.55471...</td>\n",
       "      <td>0A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature class_label\n",
       "0  [[7.2154e-06, 1.1226262854574536e-05, 9.577476...          0A\n",
       "1  [[1.25874e-05, 2.9857376931003163e-05, 4.90201...          0A\n",
       "2  [[5.00442e-05, 1.584144500100922e-05, 3.131867...          0A\n",
       "3  [[5.59316e-05, 2.868965890978838e-05, 2.260507...          0A\n",
       "4  [[2.30266e-05, 1.0840161257103143e-05, 2.55471...          0A"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "HqmrjFhq3u9i"
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from math import factorial\n",
    "\n",
    "def number_of_combinations(n, r):\n",
    "    return int(factorial(n) / (factorial(n - r) * factorial(r)))\n",
    "\n",
    "def prepare_data_pair(X, y, labels):\n",
    "    data = [[], [], []]\n",
    "    data_labels = [[], [], []]\n",
    "    \n",
    "    for label in labels:\n",
    "        label = f\"1{label}\"\n",
    "        semilabel = f\"0{label}\"\n",
    "\n",
    "        indices = np.array(list(range(len(y))))\n",
    "        similar_indices = indices[y == label]\n",
    "        train_half_size = number_of_combinations(len(similar_indices), 2)\n",
    "\n",
    "        semisimilar_indices = indices[y == semilabel][:train_half_size]\n",
    "\n",
    "        dissimilar_indices = indices[(y != label) & (y != semilabel)]\n",
    "        np.random.shuffle(dissimilar_indices)\n",
    "\n",
    "        dissimilar_indices = dissimilar_indices[:train_half_size - len(semisimilar_indices)]\n",
    "        dissimilar_indices = np.concatenate([semisimilar_indices, dissimilar_indices])\n",
    "\n",
    "        np.random.shuffle(dissimilar_indices)\n",
    "        it = iter(dissimilar_indices)\n",
    "\n",
    "        for i, j in combinations(similar_indices, 2):\n",
    "            z = next(it)\n",
    "            for index, value in enumerate([z, i, j]):\n",
    "                data[index].append(X[value])\n",
    "                data_labels[index].append(y[value])\n",
    "            \n",
    "        print(y[z], y[i], y[j])\n",
    "    \n",
    "    data = np.array(data)\n",
    "    data_labels = np.array(data_labels)\n",
    "    return data, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "qq7qH_Eh3u9m",
    "outputId": "02734d1c-465a-479e-f383-503415d5f3b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0A', '0B', '0D', '0EH', '0EL', '0G', '1A', '1B', '1D', '1EH', '1EL', '1G']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "T9FB6PBB3u9r",
    "outputId": "1759fa47-1cd9-4bad-fe2b-af5260cd2465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0EH 1EH 1EH\n",
      "1EH 1D 1D\n",
      "0D 1A 1A\n",
      "0D 1B 1B\n",
      "1A 1G 1G\n",
      "1D 1EL 1EL\n"
     ]
    }
   ],
   "source": [
    "# split the dataset \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "input_data = np.array(featuresdf.feature.tolist())\n",
    "input_labels = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# split train and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_data, input_labels, test_size=0.2, random_state = 42)\n",
    "\n",
    "# labels\n",
    "labels = [\"EH\", \"D\", \"A\", \"B\", \"G\", \"EL\"]\n",
    "\n",
    "# prepare data set pairs\n",
    "X, y = prepare_data_pair(x_train, y_train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "ZoSUP4dg3u9u",
    "outputId": "10ecf67e-7e05-4443-df05-a842c9d18b02",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 759, 64, 64), (3, 759))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "lJv29W0y3u91",
    "outputId": "192dd052-b77e-42c0-a50b-74d4e264f5d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 64, 64), (200,))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "Ri3TLncR3u94",
    "outputId": "90a6c723-2e0f-4a17-e348-8f5f5703b7a9",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51, 64, 64), (51,))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "PhF_lk3g3u98",
    "outputId": "fbdeeee8-c0b5-416e-db4b-e8e687e4aa76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0D' '1EH' '1EH']\n",
      "['0EL' '1EH' '1EH']\n",
      "['0D' '1EH' '1EH']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(y[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtEvp-x53u9_"
   },
   "source": [
    "### Convolutional Neural Network (CNN) model architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "J98TupvE3u-A"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dense, Dropout, Flatten, Lambda, LayerNormalization, Layer\n",
    "K.clear_session()\n",
    "\n",
    "def build_base_network(input_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(3, kernel_size=3, activation='relu', input_shape=input_shape, padding=\"VALID\"))\n",
    "#     model.add(LayerNormalization(axis=1))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv1D(3, kernel_size=3, activation='relu', padding=\"VALID\"))\n",
    "#     model.add(LayerNormalization(axis=1))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "#     model.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
    "# #     model.add(LayerNormalization(axis=1))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Dropout(0.2))\n",
    "    \n",
    "#     model.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
    "# #     model.add(LayerNormalization(axis=1))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Dropout(0.2))\n",
    "    \n",
    "#     model.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
    "# #     model.add(LayerNormalization(axis=1))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Dropout(0.2))\n",
    "    \n",
    "#     model.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
    "# #     model.add(LayerNormalization(axis=1))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Dropout(0.2))\n",
    "\n",
    "#     model.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
    "# #     model.add(LayerNormalization(axis=1))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Dropout(0.2))\n",
    "\n",
    "#     model.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
    "# #     model.add(LayerNormalization(axis=1))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Dropout(0.2))\n",
    "    \n",
    "#     model.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
    "# #     model.add(LayerNormalization(axis=1))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "#     model.add(Dense(1024))\n",
    "#     model.add(LayerNormalization())\n",
    "#     model.add(Dropout(0.3))\n",
    "    \n",
    "#     model.add(Dense(512))\n",
    "#     model.add(LayerNormalization())\n",
    "#     model.add(Dropout(0.3))\n",
    "    \n",
    "#     model.add(Dense(256))\n",
    "#     model.add(LayerNormalization())\n",
    "#     model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(128))\n",
    "    return model\n",
    "\n",
    "def distance(emb1, emb2):\n",
    "    return np.sum(np.square(emb1 - emb2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSgJQc8F3u-F"
   },
   "source": [
    "### Compiling the model \n",
    "\n",
    "For compiling our model, we will use the same three parameters as the previous model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "niWxBn6e3u-F"
   },
   "outputs": [],
   "source": [
    "# from model import create_model as build_base_network\n",
    "\n",
    "input_dim = X.shape[2:]\n",
    "\n",
    "# We only test DenseNet-121 in this script for demo purpose\n",
    "import tensorflow as tf\n",
    "# base_network = tf.keras.applications.DenseNet121(\n",
    "#     include_top=False, weights=None, input_tensor=None, input_shape=input_dim,\n",
    "#     pooling=\"max\", classes=128\n",
    "# )\n",
    "\n",
    "base_network = build_base_network(input_dim)\n",
    "\n",
    "audio_n = Input(shape=input_dim)\n",
    "audio_a = Input(shape=input_dim)\n",
    "audio_p = Input(shape=input_dim)\n",
    "\n",
    "feat_vecs_n = base_network(audio_n)\n",
    "feat_vecs_a = base_network(audio_a)\n",
    "feat_vecs_p = base_network(audio_p)\n",
    "\n",
    "class TripletLossLayer(Layer):\n",
    "    def __init__(self, alpha, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def triplet_loss(self, inputs):\n",
    "        a, p, n = inputs\n",
    "        p_dist = K.sum(K.square(a-p), axis=-1)\n",
    "        n_dist = K.sum(K.square(a-n), axis=-1)\n",
    "        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        loss = self.triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "        return loss\n",
    "\n",
    "# Layer that computes the triplet loss from anchor, positive and negative embedding vectors\n",
    "difference = TripletLossLayer(alpha=0.2, name='triplet_loss_layer')([feat_vecs_n, feat_vecs_a, feat_vecs_p])\n",
    "\n",
    "# initialize training params\n",
    "epochs = 128\n",
    "batch_size = 32\n",
    "\n",
    "optimizer = Adam()\n",
    "# optimizer = RMSprop()\n",
    "\n",
    "# initialize the network\n",
    "model = Model(inputs=[audio_n, audio_a, audio_p], outputs=difference)\n",
    "model.compile(loss=None, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "qgqWP5213u-J",
    "outputId": "130f9fc4-709a-4eb8-8541-8d69ce2f095c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 64, 64)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 64, 64)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 128)          6113        input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "triplet_loss_layer (TripletLoss ()                   0           sequential[0][0]                 \n",
      "                                                                 sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,113\n",
      "Trainable params: 6,113\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display model architecture summary \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "pgCkA14N3u-M",
    "outputId": "1d2a63ea-3861-449a-f882-8966d8b53e0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 62, 3)             579       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 31, 3)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 29, 3)             30        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 14, 3)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 3)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 42)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               5504      \n",
      "=================================================================\n",
      "Total params: 6,113\n",
      "Trainable params: 6,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.layers[3].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixLSgTDl3u-P"
   },
   "source": [
    "### Training \n",
    "\n",
    "Here we will train the model. As training a CNN can take a sigificant amount of time, we will start with a low number of epochs and a low batch size. If we can see from the output that the model is converging, we will increase both numbers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "rvVo5MbA3u-Q",
    "outputId": "cb3f5a7c-c9cc-4dba-847d-999509bfb5f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 6.3222 - val_loss: 6.3334\n",
      "Epoch 2/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3224 - val_loss: 6.3333\n",
      "Epoch 3/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 4/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 5/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 6/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 7/128\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 8/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 9/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 10/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 11/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 12/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 13/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 14/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 15/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 16/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 17/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 18/128\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 19/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 20/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 21/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 22/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 23/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 24/128\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 25/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 26/128\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 27/128\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 28/128\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 29/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 30/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 31/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 32/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 33/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 34/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 35/128\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 36/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 37/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 38/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 39/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 40/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 41/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 42/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 43/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 44/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 45/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 46/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 47/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 48/128\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 49/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 50/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 51/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 52/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 53/128\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 54/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 55/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 56/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 57/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 58/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 59/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 60/128\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 61/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 62/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 63/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 64/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 65/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 66/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 67/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 68/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 69/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 70/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 71/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 72/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 73/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 74/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 75/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 76/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 77/128\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 78/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 79/128\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 80/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 81/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 82/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 83/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 84/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 85/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 86/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 87/128\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 88/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 89/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 90/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 91/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 92/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 93/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 94/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 95/128\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 96/128\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 97/128\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 98/128\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 99/128\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 100/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 101/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 102/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 103/128\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 104/128\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 105/128\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 106/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 107/128\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 108/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 109/128\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 110/128\n",
      "18/18 [==============================] - ETA: 0s - loss: 6.400 - 0s 16ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 111/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 112/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 113/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 114/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 115/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 116/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 117/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 118/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 119/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 120/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 121/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 122/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 123/128\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 124/128\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 125/128\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 126/128\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 127/128\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Epoch 128/128\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 6.3222 - val_loss: 6.3333\n",
      "Training completed in time:  0.6204944372177124 min\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "from time import time\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath='saved_models/weights.best.basic_cnn.hdf5', \n",
    "    verbose=1, \n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "start = time()\n",
    "model.fit(\n",
    "    [X[0], X[1], X[2]], \n",
    "    None, \n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs, \n",
    "    validation_split=0.25,\n",
    "#     callbacks=[checkpointer], \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "duration = (time() - start)/60\n",
    "print(\"Training completed in time: \", duration, \"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-n0tMCh3u-Y"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6GmeWNQ3u-Y"
   },
   "source": [
    "### Best freq treshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "yjOou5Gu3u-a",
    "outputId": "12f7cdf8-da93-4502-8187-8949147d0f20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-130-6a3769ef6175>:30: RuntimeWarning: invalid value encountered in true_divide\n",
      "  thresholds = np.arange(min_threshold, max_threshold, threshold_step)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arange: cannot compute length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-6a3769ef6175>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mmax_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mthreshold_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_threshold\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmin_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mthresholds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mf1_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentical\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistances\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arange: cannot compute length"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "use_test = False\n",
    "\n",
    "distances = [] # squared L2 distance between pairs\n",
    "identical = [] # 1 if same identity, 0 otherwise\n",
    "\n",
    "if use_test:\n",
    "    num = len(x_test)\n",
    "    embedded = model.layers[3].predict(x_test)\n",
    "    \n",
    "else:\n",
    "    num = len(x_train)\n",
    "    embedded = model.layers[3].predict(x_train)\n",
    "\n",
    "for i in range(num):\n",
    "    for j in range(num):\n",
    "        distances.append(distance(embedded[i], embedded[j]))\n",
    "        if use_test:\n",
    "            identical.append(1 if y_test[i] == y_test[j] else 0)\n",
    "        else:\n",
    "            identical.append(1 if y_train[i] == y_train[j] else 0)\n",
    "            \n",
    "distances = np.array(distances)\n",
    "identical = np.array(identical)\n",
    "\n",
    "min_threshold = min(distances)\n",
    "max_threshold = max(distances)\n",
    "threshold_step = (max_threshold - min_threshold)/1000\n",
    "thresholds = np.arange(min_threshold, max_threshold, threshold_step)\n",
    "\n",
    "f1_scores = [f1_score(identical, distances < t) for t in thresholds]\n",
    "acc_scores = [accuracy_score(identical, distances < t) for t in thresholds]\n",
    "\n",
    "# max f1\n",
    "opt_idx = np.argmax(f1_scores)\n",
    "opt_f1 = np.max(f1_scores)\n",
    "\n",
    "# Threshold at maximal F1 score\n",
    "opt_tau = thresholds[opt_idx]\n",
    "\n",
    "# Accuracy at maximal F1 score\n",
    "opt_acc = accuracy_score(identical, distances < opt_tau)\n",
    "\n",
    "# Plot F1 score and accuracy as function of distance threshold\n",
    "plt.plot(thresholds, f1_scores, label='F1 score');\n",
    "plt.plot(thresholds, acc_scores, label='Accuracy');\n",
    "plt.axvline(x=opt_tau, linestyle='--', lw=1, c='lightgrey', label='Threshold')\n",
    "plt.title(f'Max: Acc={opt_acc:.2f}, f1={opt_f1:.2f} at threshold {opt_tau:.8f}');\n",
    "plt.xlabel('Distance threshold')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6kh4vMb3u-f",
    "outputId": "1f655966-f0a8-4924-93e4-4a3f6c1dfb09"
   },
   "outputs": [],
   "source": [
    "dist_pos = distances[identical == 1]\n",
    "dist_neg = distances[identical == 0]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.hist(dist_pos)\n",
    "plt.axvline(x=opt_tau, linestyle='--', lw=1, c='lightgrey', label='Threshold')\n",
    "plt.title('Distances (pos. pairs)')\n",
    "plt.legend();\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(dist_neg)\n",
    "plt.axvline(x=opt_tau, linestyle='--', lw=1, c='lightgrey', label='Threshold')\n",
    "plt.title('Distances (neg. pairs)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOMgyhXR3u-j"
   },
   "source": [
    "### Testing Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "rGOEbCnt3u-k",
    "outputId": "fceebb3f-0dff-4f2e-9872-3be7e228020f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.8723567858515956\n",
      "\n",
      "classification_report\n",
      "========================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      2320\n",
      "           1       0.39      0.32      0.35       281\n",
      "\n",
      "    accuracy                           0.87      2601\n",
      "   macro avg       0.65      0.63      0.64      2601\n",
      "weighted avg       0.86      0.87      0.87      2601\n",
      "\n",
      "\n",
      "confusion matrix\n",
      "========================\n",
      "[[2180  140]\n",
      " [ 192   89]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "distances = [] # squared L2 distance between pairs\n",
    "identical = [] # 1 if same identity, 0 otherwise\n",
    "prediction = []\n",
    "\n",
    "num = len(x_test)\n",
    "embedded = model.layers[3].predict(x_test)\n",
    "\n",
    "for i in range(num):\n",
    "    for j in range(num):\n",
    "        distances.append(distance(embedded[i], embedded[j]))\n",
    "        identical.append(1 if y_test[i] == y_test[j] else 0)\n",
    "        prediction.append(1 if distances[-1] < opt_tau else 0)\n",
    "        \n",
    "distances = np.array(distances)\n",
    "identical = np.array(identical)\n",
    "prediction = np.array(prediction)\n",
    "\n",
    "print(\"accuracy =\", accuracy_score(identical, prediction))\n",
    "\n",
    "print(\"\\nclassification_report\")\n",
    "print(\"========================\")\n",
    "print(classification_report(identical, prediction))\n",
    "\n",
    "print(\"\\nconfusion matrix\")\n",
    "print(\"========================\")\n",
    "print(confusion_matrix(identical, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCuryhb33u-n"
   },
   "source": [
    "### Testing (new data) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "36ddc801b7524eab81e40dcf39c8c0d2"
     ]
    },
    "id": "GXxLglE33u-p",
    "outputId": "adb9efe7-5fae-4b5f-ed4b-e7e8f1e804e4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54b3c1d84f242e4acf6b0ff797fdb61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A, \n",
      "\n",
      "(10, 40, 1)\n"
     ]
    }
   ],
   "source": [
    "x_test_sample = []\n",
    "y_test_sample = []\n",
    "\n",
    "# sample_dirs = [\"EH\", \"D\", \"A\", \"B\", \"G\", \"EL\"]\n",
    "sample_dirs = [\"A\"]\n",
    "\n",
    "sample_label = \"A\"\n",
    "sample_filepath = os.path.join(\"data\", \"sampleA.wav\")\n",
    "sample_vector = extract_features(sample_filepath).reshape((1, -1, 1))\n",
    "sample_embedded = model.layers[3].predict(sample_vector)\n",
    "\n",
    "for label in tqdm(sample_dirs):\n",
    "    print(label, end=\", \")\n",
    "    labeldir= os.path.join(\"data\", \"old_guitar_sample\", label)\n",
    "\n",
    "    for filename in (os.listdir(labeldir)):\n",
    "        anchor_filepath = os.path.join(\"data\", \"old_guitar_sample\", label, filename)\n",
    "        \n",
    "        x_test_sample.append(extract_features(anchor_filepath).reshape((-1, 1)))\n",
    "        y_test_sample.append(label)\n",
    "\n",
    "print()\n",
    "x_test_sample = np.array(x_test_sample)\n",
    "y_test_sample = np.array(y_test_sample)\n",
    "\n",
    "print(x_test_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "-Y3kHXH-3u-s",
    "outputId": "cb6b75a1-0c58-458b-e4a8-e90a6739e5a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.18\n",
      "\n",
      "classification_report\n",
      "========================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.18      0.31       100\n",
      "\n",
      "    accuracy                           0.18       100\n",
      "   macro avg       0.50      0.09      0.15       100\n",
      "weighted avg       1.00      0.18      0.31       100\n",
      "\n",
      "\n",
      "confusion matrix\n",
      "========================\n",
      "[[ 0  0]\n",
      " [82 18]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\christian\\.virtualenvs\\guitar_music_note_recognizer_notebook\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "distances = [] # squared L2 distance between pairs\n",
    "identical = [] # 1 if same identity, 0 otherwise\n",
    "prediction = []\n",
    "\n",
    "num = len(x_test_sample)\n",
    "embedded = model.layers[3].predict(x_test_sample)\n",
    "\n",
    "for i in range(num):\n",
    "    for j in range(num):\n",
    "        distances.append(distance(embedded[i], embedded[j]))\n",
    "        identical.append(1 if y_test_sample[i] == y_test_sample[j] else 0)\n",
    "        prediction.append(1 if distances[-1] < opt_tau else 0)\n",
    "        \n",
    "distances = np.array(distances)\n",
    "identical = np.array(identical)\n",
    "prediction = np.array(prediction)\n",
    "\n",
    "print(\"accuracy =\", accuracy_score(identical, prediction))\n",
    "\n",
    "print(\"\\nclassification_report\")\n",
    "print(\"========================\")\n",
    "print(classification_report(identical, prediction))\n",
    "\n",
    "print(\"\\nconfusion matrix\")\n",
    "print(\"========================\")\n",
    "print(confusion_matrix(identical, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LuC2AcO3u-w"
   },
   "source": [
    "### Testing (sample) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      ""
     ]
    },
    "id": "5AMEATl53u-w",
    "outputId": "cff24703-b5d2-46a0-95cd-2a17eaef1bd0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\christian\\.virtualenvs\\guitar_music_note_recognizer_notebook\\lib\\site-packages\\librosa\\core\\audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encountered while parsing file:  G Sample 9.wav\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-195-750767786edf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msample_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"A\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msample_filepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/content/drive/My Drive/Colab Notebooks/data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sampleA.wav\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0msample_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0msample_embedded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "x_test_sample = []\n",
    "y_test_sample = []\n",
    "\n",
    "sample_dirs = [\"1EH\", \"1D\", \"1A\", \"1B\", \"1G\", \"1EL\"]\n",
    "\n",
    "sample_label = \"A\"\n",
    "sample_filepath = os.path.join(\"/content/drive/My Drive/Colab Notebooks/data\", \"sampleA.wav\")\n",
    "sample_vector = extract_features(sample_filepath).reshape((1, -1, 1))\n",
    "sample_embedded = model.layers[3].predict(sample_vector)\n",
    "\n",
    "for label in tqdm(sample_dirs):\n",
    "    print(label, end=\", \")\n",
    "    labeldir= os.path.join(DATA_DIR, label)\n",
    "\n",
    "    for filename in (os.listdir(labeldir)):\n",
    "        anchor_filepath = os.path.join(DATA_DIR, label, filename)\n",
    "        \n",
    "        x_test_sample.append(extract_features(anchor_filepath).reshape((-1, 1)))\n",
    "        y_test_sample.append(1 if label.startswith(\"1\") and label.endswith(sample_label) else 0)\n",
    "\n",
    "print()\n",
    "x_test_sample = np.array(x_test_sample)\n",
    "y_test_sample = np.array(y_test_sample)\n",
    "\n",
    "print(x_test_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4omNHOQ3u-1",
    "outputId": "f640715d-b1e0-4685-ee26-4064bdcdcc74"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "distances = [] # squared L2 distance between pairs\n",
    "prediction = []\n",
    "\n",
    "num = len(x_test_sample)\n",
    "embedded = model.layers[3].predict(x_test_sample)\n",
    "\n",
    "for i in range(num):\n",
    "    distances.append(distance(embedded[i], sample_embedded))\n",
    "    prediction.append(1 if distances[-1] < opt_tau else 0)\n",
    "\n",
    "identical = y_test_sample\n",
    "distances = np.array(distances)\n",
    "prediction = np.array(prediction)\n",
    "\n",
    "print(\"accuracy =\", accuracy_score(identical, prediction))\n",
    "\n",
    "print(\"\\nclassification_report\")\n",
    "print(\"========================\")\n",
    "print(classification_report(identical, prediction))\n",
    "\n",
    "print(\"\\nconfusion matrix\")\n",
    "print(\"========================\")\n",
    "print(confusion_matrix(identical, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXy-pxCh3u-4",
    "outputId": "72300e2d-ad6b-4897-be37-1cdae1cd69f0",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118,), (118,))"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape, identical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "FuE-EUf43u-8"
   },
   "outputs": [],
   "source": [
    "# weights path\n",
    "# weights_path = os.path.join(\"C:\\\\Users\\\\christian\\\\Documents\\\\christian\\\\work\\\\python\\\\guitar_music_note_recognizer\\\\music_note_recognizer\\\\static\\\\music_note_recognizer\\\\weights\", f'{label}_weights.h5')\n",
    "# weights_path = \"saved_models/triplet_128_32.h5\"\n",
    "\n",
    "# load weights\n",
    "# model.save_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JzbHvLB3u--"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Triplet Siamese Network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01870c47bcd4473ea6d4a75a5671983c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "02fa5eedbf8c403c94e45735e8e2072e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0fb775d8199642539df1f477850b9bb8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32033641990b41f0bbee1cb4eb29f795": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab984203a3b64540aaf15530cd751c56",
      "placeholder": "",
      "style": "IPY_MODEL_01870c47bcd4473ea6d4a75a5671983c",
      "value": " 0/12 [00:00&lt;?, ?it/s]"
     }
    },
    "8b729491eac44963bc4949b2d3ffda1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ab984203a3b64540aaf15530cd751c56": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc12aa92344d4fd885d91aafeda3da2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "  0%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fb775d8199642539df1f477850b9bb8",
      "max": 12,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b729491eac44963bc4949b2d3ffda1a",
      "value": 0
     }
    },
    "e46f85e99e09488bae8eb6d70666e4ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc12aa92344d4fd885d91aafeda3da2a",
       "IPY_MODEL_32033641990b41f0bbee1cb4eb29f795"
      ],
      "layout": "IPY_MODEL_02fa5eedbf8c403c94e45735e8e2072e"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
